{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eadf0c8-6a9b-45cc-923b-08f7fae043a3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T13:52:51.237925Z",
     "iopub.status.busy": "2021-11-10T13:52:51.237280Z",
     "iopub.status.idle": "2021-11-10T13:52:51.244228Z",
     "shell.execute_reply": "2021-11-10T13:52:51.243424Z",
     "shell.execute_reply.started": "2021-11-10T13:52:51.237518Z"
    }
   },
   "source": [
    "# IMPERIAL COLLEGE LONDON\n",
    "# MSc ASSESSMENT 2021/22"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "married-qualification",
   "metadata": {},
   "source": [
    "## Environmental Data Module Week 1: Data Science and Machine Learning\n",
    "\n",
    "***For internal students of Imperial College London***<br>\n",
    "***Taken by students of MSc Environmental Data Science and Machine Learning***\n",
    "\n",
    "13:00 ‚Äì 16:00 London Time, Friday 19 November 2021\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c9d2a9-840d-4ae5-a165-9ff89eaee833",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-11-10T14:04:34.022011Z",
     "iopub.status.busy": "2021-11-10T14:04:34.021786Z",
     "iopub.status.idle": "2021-11-10T14:04:34.033451Z",
     "shell.execute_reply": "2021-11-10T14:04:34.032087Z",
     "shell.execute_reply.started": "2021-11-10T14:04:34.021985Z"
    }
   },
   "source": [
    "## ‚ö†Ô∏è Disclaimer ‚ö†Ô∏è\n",
    "\n",
    "### Assessment type\n",
    "The assessments are run as open-book assessments, and as such we have worked hard to create a coursework that assesses synthesis of knowledge rather than factual recall. Be aware that access to the internet,\n",
    "notes or other sources of factual information in the time provided may not be too helpful and may well\n",
    "limit your time to successfully synthesise the answers required. \n",
    "\n",
    "### Plagiarism\n",
    "The use of the work of another student,\n",
    "past or present, constitutes plagiarism. Giving your work to another student to use may also constitute\n",
    "an offence. Collusion is a form of plagiarism and will be treated in a similar manner. This is an individual\n",
    "assessment and thus should be completed solely by you. The College will investigate all instances where\n",
    "an assessment offence is reported or suspected, using plagiarism software, vivas and other tools, and apply\n",
    "appropriate penalties to students. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vocal-shanghai",
   "metadata": {},
   "source": [
    "## Submission of your assessment\n",
    "You have ***until 4 pm London time on Friday, November 19th***, to complete this assessment. Code pushed after the deadline or sent via email/Teams will not be taken into consideration unless there are very good mitigating circumstances. We recommend pushing intermediate solutions of your code often.\n",
    "\n",
    "To submit you need to do a two step process:\n",
    "\n",
    "1. First, **run the code in the very last cell of this notebook**. This will do three things: \n",
    "    * It will ensure that you have the correct variable names declared in the notebook. If you don't have the right variable names, the code will return a `NameError` and this will inform you of what variable is missing. Look in the notebook to see where you were supposed to declare it.\n",
    "    * If all your variables are declared, the code should output a list of tests on the **type** of the variables you declared. If all the types are correct, you will see green `PASSED` tests. Keep in mind, this only indicates that you have the correct type, **NOT that your solution is correct**. It is possible to still have a correct solution and fail the **type check**, for instance, if your solution is very different than mine and you end up with a different class than what I expect. So check your solution, make sure it is ok, but don't panic if you cannot see a problem.\n",
    "    * The code will save your solution in the 'answers' folder so it can easily be extracted for marking. The message will tell you whether or not the variables were correctly saved (even if they were not of the right type)\n",
    "2. Second, if you have all PASSED tests or if you are confident that your solution is correct, **YOU NEED TO PUSH YOUR ASSESSMENT TO GitHub Classroom!** Code not pushed to GitHub classroom before the deadline will not be taken into consideration. Note that if you make changes to your code, you need to do step (1) again (run the code at the end of this notebook) and then step (2) again (push to GitHub).\n",
    "\n",
    "You can push to GitHub as often you want during the assessment (only the last push before the deadline counts): so don't leave it to the last minute. \n",
    "\n",
    "## Assessed Coursework Structure and marking criteria\n",
    "This assessment is marked out of a **maximum of 100 marks**. The total number of achievable point per question is clearly indicated. You **need to answer all questions** to achieve the maximum mark. \n",
    "\n",
    "### Assessment Criteria\n",
    "In all assessments, we will analyse performance against performance\n",
    "on the rest of the course and against data from previous years and use an evidence-based approach to\n",
    "maintain a fair and robust assessment. As with all assessments, the best strategy is to read the question\n",
    "carefully and answer as fully as possible, taking account of the time and number of marks available.\n",
    "The following will be used to attribute marks:\n",
    "- **Clean code**: Follows general clean code pracices, e.g. code is well organised and easy to read and understand, avoids repetition of code by creating functions where needed (DRY principle), code can be executed top to bottom in the notebook, variable names are logical. For each question below, **20% of the mark will be awarded for how clean/well organised your code is.**\n",
    "- **Correct solution**: A solution that gives a correct answer\n",
    "- **Complete solution**: Ability to show that you have understood the general principles behind the question by fully testing your answer, for instance, by plotting relevant data distribution or using other tools to gain insight from your data. Be careful though not to overdo it: remember, clean code!\n",
    "\n",
    "\n",
    "\n",
    "## üçÄ GOOD LUCK!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274f388b-0818-4b08-ab80-95288f632be8",
   "metadata": {},
   "source": [
    "# INITIAL SETUP\n",
    "\n",
    "üö® Please enter your **CID** (`int`) and **GitHub Username** (`string`) in the cell below, and **run the cell (execute the code)**. Double check that it is correct! üö®"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea9ac08-90cb-42d6-aed8-32b04b02ba4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T08:34:18.469658Z",
     "start_time": "2021-11-18T08:34:18.452410Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CID = \n",
    "GitHubUsername = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84a6843-c600-4293-bae5-8623d5f76f46",
   "metadata": {},
   "source": [
    "---------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841ab29-b979-4ca3-b95a-fadce992f452",
   "metadata": {},
   "source": [
    "# üö¢PART A: Data preparation - core data from ODP [50 marks]\n",
    "\n",
    "Execute the cell below to load the data into a variable named `core_df`. Questions 1 to 5 are based on this data. Unless otherwise stated, always save your dataframe as `code_df` after making a transformation to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3488bdb0-1c0d-4df9-b65d-84eebd470341",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "core_df = pd.read_csv('data/core_data_labelled.csv')\n",
    "core_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a30cbb-d689-4d58-9e57-5d0cba3c4310",
   "metadata": {},
   "source": [
    "\n",
    "## The Data\n",
    "You should already be familiar with the type of data here because it is ODP core data:\n",
    "\n",
    "- **'Leg'**: The 'Leg' number, which is the old ODP (the predecessor of IODP) terminology for an 'Expedition'\n",
    "- **'Site'**: The ODP Site number, where multiple wells can be drilled\n",
    "- **'H'**: The 'Hole' name (a sequential letter starting with A, B, C, ...), effectively a well. So a full well name would be 'leg-siteHole', such as 'ODP 194-1192A' \n",
    "- **'Cor'**: The core number, each core is drilled for about 9.8 meters down from the surface of the sediments. Thus, core 1 is the shallowest core, and core numbers increase downhole.\n",
    "- **'T'**: The tool used to cut the core, i.e. the cutting shoe. This can be one of 6 types: H, X, R, Z, M and W. H and X are the most common.\n",
    "- **'Sc'**: The section of the core the sample comes from. Each core is divided into up to 7 sections and one core catcher ('CC').\n",
    "- **'Top(cm)'**: The distance in cm from the top of the section where the sample was collected.\n",
    "- **'Depth (mbsf)'**: The depth of the sample in the well measured from the seafloor (mbsf = meters below seafloor).\n",
    "- **'Corr. Counts'**: The 'Corrected counts' for the core natural gamma ray. In effect, natural gamma ray data.\n",
    "- **'Density (g/cc)'**: An estimate of the bulk density of the rock and sediments from automated core measurements.\n",
    "- **'L*'**: The luminosity channel, part of the Lab* color space of the sediment color measured by the automated tracks. \n",
    "- **'a*'**: The 'a' color axis, part of the Lab* color space of the sediment color measured by the automated tracks.\n",
    "- **'b*'**: The 'b' color axis, part of the Lab* color space of the sediment color measured by the automated tracks.\n",
    "- **'CaCO3 (wt %)'**: percent carbonate present in the sample. \n",
    "\n",
    "## Answer the following questions using this data:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440444ce-4970-4b98-8abb-894daa9034c6",
   "metadata": {},
   "source": [
    "# Question 1 [10 marks]\n",
    "**a)** What is the total number of missing values in `core_df`? Save your answer into a variable named `cdf_nb_missing` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49571b60-f635-4d75-b0ec-16d1ddd4bab5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Your Answer:\n",
    "cdf_nb_missing = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac521b5-1d8b-460a-9a09-c8ccd1914ec6",
   "metadata": {},
   "source": [
    "**b)** What is the average of the standard deviations of all of the numerical features in `core_df`? Save your answer into a variable named `cdf_std_mean` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f850ffa7-dd02-4dfa-b15a-10b362e8ee34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cdf_std_mean = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce79abf-e32a-49a3-af3a-a468993280d2",
   "metadata": {},
   "source": [
    "**c)** What are the maximum and minimum values of `CaCO3 (wt %)`  in `core_df`? Save your answers into a variable named `cdf_min_carb` and `cdf_max_carb` below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9536a8c7-6bb4-46fa-9cdb-6d625f940b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdf_min_carb = \n",
    "cdf_max_carb = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d0f0c7-c81b-4616-a00d-efa4c25c2a65",
   "metadata": {},
   "source": [
    "# Question 2 [10 marks]\n",
    "Select all numerical columns from `core_df` and save them into an array variable called `num_columns`. Use a single imputer to replace all missing values from `num_columns` straight in `core_df` and choose either the `mean` or the `median` as your strategy. Your goal is to have values as close as possible from the most frequent value (but not exactly the most frequent: **do not** use the `most_frequent` strategy). In other words, choose the most appropriate strategy between  `mean` or `median` for the data that you have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41213626-56ce-4029-ace0-fbb6db8db913",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "num_columns = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74054de-a5ea-4a4c-ac64-15407e86e36b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "core_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f29744-9cec-4b13-a105-cfcdca37027d",
   "metadata": {},
   "source": [
    "# Question 3 [10 marks]\n",
    "Imput any missing categorical value with a reasonable strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e693b57d-dc93-4828-915c-4d2eedbb80d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "core_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2414134c-a2ee-4455-8d3a-c85758c9ad2c",
   "metadata": {},
   "source": [
    "# Question 4 [10 marks]\n",
    "Are there any obvious numerical outliers (i.e. data that clearly need to be measurement errors) from `core_df`? Give your answer below as a string (either `'yes'` or `'no'`) in the variable named `outliers_present`. If there are any, then **drop the rows** containing the outliers directly in `core_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496d46dd-4470-43dc-baaf-4a94fe1995fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers_present = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e434051c-bfd7-4b1e-8b9a-16f24019e6d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "core_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14f0951-77f4-4c00-997d-9525e0d82f61",
   "metadata": {},
   "source": [
    "# Question 5 [10 marks]\n",
    "**a)** First run the code below to create a copy of your dataframe called `encoded_df`. For **Question 5**, you will use this new dataframe instead of the original `core_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4c229a-35fc-4819-8105-15c798a5e18a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this code:\n",
    "\n",
    "encoded_df = core_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f7dafa-89d2-499a-8b5c-e05f478a9a87",
   "metadata": {},
   "source": [
    "**b)** Now look carefully at each one of the features in `core_df`. Do the following:\n",
    "* Scale numerical features that need scaling using a single scaler (decide which scaler is most appropriate - **don't use a different scaler for each numerical feature**) \n",
    "* Encode all categorical features. \n",
    "\n",
    "Replace the original values directly in `encoded_df` by their encoded/scaled values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e53f9c-087e-4a34-8578-55c9781a4c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Your Code here\n",
    "encoded_df = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df26fdc5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2021-11-16T15:50:20.491120Z",
     "iopub.status.idle": "2021-11-16T15:50:20.491472Z",
     "shell.execute_reply": "2021-11-16T15:50:20.491321Z",
     "shell.execute_reply.started": "2021-11-16T15:50:20.491307Z"
    },
    "tags": []
   },
   "source": [
    "---------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30be0e2-7de1-4391-8a06-a9b3a3eced16",
   "metadata": {},
   "source": [
    "# üêù PART B: Training and testing algorithms - Swarm Behaviour\n",
    "\n",
    "Execute the cell below to load the labelled data into a variable named `data`. **Questions 6** is based on this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a93b799-cda6-4c2c-8749-81a09278da66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-15T13:51:28.764234Z",
     "start_time": "2021-11-15T13:51:27.640309Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = pd.read_csv('data/swarm_training.csv')\n",
    "unknown_behaviour =  pd.read_csv('data/unknown_swarm_behaviour.csv')\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c0038b-8839-48c6-bfc0-1d4bd03d2217",
   "metadata": {},
   "source": [
    "## Dataset Description\n",
    "\n",
    "The dataset contains 292 `features` (different measurement of behaviour) and 1 label (`Swarm_Behaviour`).\n",
    "\n",
    "`Swarm_Behaviour` can take the values 0 (*not swarming*) or 1 (*swarming*).\n",
    "\n",
    "The following has been done on this dataset already:\n",
    "* There is no missing data\n",
    "* Numerical values were scaled using a `RobustScaler()` \n",
    "* Categorical data are encoded\n",
    "\n",
    "Hence, the data is ready for machine learning applications though it is always possible for you to do more on the data if you want. üßΩ\n",
    "\n",
    "# Question 6 [50 marks]\n",
    "\n",
    "You are a data scientist working for a non-profit looking at endangered eco-systems. You are given the dataset above, and are asked to train a `LogisticRegression` model to predict `Swarm_Behaviour` in new, unseen samples. In insects and birds, 'swarming' refers to the tendency to form large groups when flying together.\n",
    "\n",
    "Your bosses also give you the following directives:\n",
    "* The model needs to be a `LogisticRegression` model - Upper management will discard any other approach \n",
    "* You need to train your model to achieve the highest possible **precision** using the labelled dataset (`data`) above \n",
    "* However, management requires your algorithm to have a **recall** of ***at least 70%***\n",
    "* Once you are happy with your final trained algorithm, you are asked to save it into a variable named `final_model` \n",
    "* You are then asked to make predictions about the swarming behaviour of unclassified species given in the `unknown_behaviour` dataframe above, and to save your predictions in a variable named `predictions`\n",
    "* Your algorithm will be evaluated based on its performanced on the unseen dataset (i.e. your `predictions`)\n",
    "\n",
    "GOOD LUCK! üßß"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fe103a-5fa1-46e8-a560-f84038b8c126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code\n",
    "\n",
    "final_model = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd60bd8-c742-404b-a220-10e5f06084a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db216522",
   "metadata": {},
   "source": [
    "# üö® CHECK AND SAVE YOUR ANSWERS BEFORE PUSHING üö®\n",
    "### Run the code cell below\n",
    "\n",
    "Running the cell below will save your answer(s) in the `answers` folder. This is an important part of the correction process as we will use this as a first pass assessement of your work. It will also give you an indication if you have saved all of the important variables in the notebook correctly, and if some are missing.\n",
    "\n",
    "**If you have a variable missing** the code below will output a clear, red error with a 'NameError', something like that:\n",
    "`name 'final_model' is not defined`\n",
    "\n",
    "If all of the variables are defined, the code will give you a detailed report on the expected types of each variable, and whether you need to check them or not. The variables with correct types will be saved on the disk.\n",
    "\n",
    "You can run this code as often as want, and you should run it one last time before you push your code to GitHub classroom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f30eab0-de1f-44ee-925c-447e27d74e64",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-11-18T08:36:28.697800Z",
     "start_time": "2021-11-18T08:36:28.664858Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from answers.check_answers import CheckAnswers\n",
    "\n",
    "answer = CheckAnswers(cid=CID,\n",
    "                 username=GitHubUsername,\n",
    "                 cdf_nb_missing=cdf_nb_missing,\n",
    "                cdf_std_mean=cdf_std_mean,\n",
    "                cdf_min_carb=cdf_min_carb,\n",
    "                cdf_max_carb=cdf_max_carb,\n",
    "                num_columns=num_columns,\n",
    "                outliers_present=outliers_present,\n",
    "                core_df=core_df,\n",
    "                encoded_df=encoded_df,\n",
    "                final_model=final_model,\n",
    "                predictions=predictions).checkAll()\n",
    "\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "edsml",
   "language": "python",
   "name": "edsml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
